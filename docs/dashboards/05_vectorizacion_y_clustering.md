# ğŸ“ GuÃ­a Formativa: VectorizaciÃ³n y Clustering de Documentosï»¿â€‹â€Œâ€‹â€Œâ€‹â€Œâ€‹â€‹â€â€‹â€Œâ€‹â€‹â€‹â€Œâ€‹â€Œâ€â€‹â€‹â€Œâ€Œâ€Œâ€‹â€Œâ€‹â€â€‹â€‹â€Œâ€Œâ€‹â€Œâ€‹â€‹â€â€‹â€Œâ€Œâ€‹â€‹â€Œâ€‹â€Œâ€â€‹â€‹â€Œâ€Œâ€Œâ€‹â€‹â€‹â€â€‹â€Œâ€Œâ€‹â€‹â€Œâ€‹â€‹â€â€‹â€‹â€Œâ€Œâ€Œâ€‹â€‹â€Œâ€â€‹â€Œâ€Œâ€‹â€‹â€‹â€Œâ€‹â€â€‹â€‹â€Œâ€Œâ€‹â€‹â€‹â€Œâ€â€‹â€Œâ€Œâ€‹â€‹â€‹â€‹â€Œâ€â€‹â€‹â€Œâ€Œâ€‹â€Œâ€‹â€Œâ€â€‹â€Œâ€Œâ€‹â€‹â€Œâ€Œâ€‹â€â€‹â€‹â€Œâ€Œâ€‹â€Œâ€Œâ€‹â€â€‹â€Œâ€Œâ€‹â€‹â€Œâ€‹â€Œâ€â€‹â€‹â€Œâ€Œâ€‹â€Œâ€Œâ€Œâ€â€‹â€Œâ€Œâ€‹â€‹â€‹â€Œâ€Œâ€â€‹â€‹â€Œâ€Œâ€‹â€‹â€Œâ€Œâ€â€‹â€Œâ€Œâ€‹â€‹â€Œâ€‹â€‹â€â€‹â€‹â€Œâ€Œâ€Œâ€‹â€Œâ€‹â€â€‹â€‹â€Œâ€Œâ€‹â€‹â€Œâ€‹â€â€‹â€‹â€Œâ€Œâ€‹â€‹â€‹â€‹â€â€‹â€‹â€Œâ€Œâ€‹â€‹â€Œâ€‹â€â€‹â€‹â€Œâ€Œâ€‹â€Œâ€Œâ€‹â€â€‹â€‹â€Œâ€Œâ€‹â€‹â€‹â€‹â€â€‹â€‹â€Œâ€Œâ€‹â€‹â€Œâ€‹â€â€‹â€‹â€Œâ€Œâ€‹â€‹â€‹â€Œâ€â€‹â€‹â€Œâ€Œâ€‹â€‹â€Œâ€Œâ€â€‹â€‹â€Œâ€Œâ€Œâ€‹â€Œâ€‹â€â€‹â€‹â€Œâ€Œâ€Œâ€‹â€‹â€Œâ€â€‹â€‹â€Œâ€Œâ€Œâ€‹â€‹â€‹â€â€‹â€Œâ€Œâ€‹â€‹â€Œâ€‹â€Œâ€â€‹â€‹â€Œâ€Œâ€‹â€‹â€‹â€Œâ€â€‹â€‹â€Œâ€Œâ€‹â€Œâ€‹â€‹â€â€‹â€‹â€Œâ€Œâ€‹â€‹â€Œâ€‹â€â€‹â€Œâ€Œâ€‹â€‹â€‹â€‹â€Œâ€â€‹â€Œâ€Œâ€‹â€‹â€‹â€Œâ€‹ï»¿

## ğŸš€ AnÃ¡lisis de TÃ³picos con Inteligencia Artificial (NLP & Machine Learning)

---

> ### ğŸ“ InformaciÃ³n de CertificaciÃ³n y Referencia
>
> **Autor original/Referencia:** @TodoEconometria  ï»¿â€‹â€Œâ€‹â€Œâ€‹â€Œâ€‹â€‹â€â€‹â€Œâ€‹â€‹â€‹â€Œâ€‹â€Œâ€â€‹â€‹â€Œâ€Œâ€Œâ€‹â€Œâ€‹â€â€‹â€‹â€Œâ€Œâ€‹â€Œâ€‹â€‹â€â€‹â€Œâ€Œâ€‹â€‹â€Œâ€‹â€Œâ€â€‹â€‹â€Œâ€Œâ€Œâ€‹â€‹â€‹â€â€‹â€Œâ€Œâ€‹â€‹â€Œâ€‹â€‹â€â€‹â€‹â€Œâ€Œâ€Œâ€‹â€‹â€Œâ€â€‹â€Œâ€Œâ€‹â€‹â€‹â€Œâ€‹â€â€‹â€‹â€Œâ€Œâ€‹â€‹â€‹â€Œâ€â€‹â€Œâ€Œâ€‹â€‹â€‹â€‹â€Œâ€â€‹â€‹â€Œâ€Œâ€‹â€Œâ€‹â€Œâ€â€‹â€Œâ€Œâ€‹â€‹â€Œâ€Œâ€‹â€â€‹â€‹â€Œâ€Œâ€‹â€Œâ€Œâ€‹â€â€‹â€Œâ€Œâ€‹â€‹â€Œâ€‹â€Œâ€â€‹â€‹â€Œâ€Œâ€‹â€Œâ€Œâ€Œâ€â€‹â€Œâ€Œâ€‹â€‹â€‹â€Œâ€Œâ€â€‹â€‹â€Œâ€Œâ€‹â€‹â€Œâ€Œâ€â€‹â€Œâ€Œâ€‹â€‹â€Œâ€‹â€‹â€â€‹â€‹â€Œâ€Œâ€Œâ€‹â€Œâ€‹â€â€‹â€‹â€Œâ€Œâ€‹â€‹â€Œâ€‹â€â€‹â€‹â€Œâ€Œâ€‹â€‹â€‹â€‹â€â€‹â€‹â€Œâ€Œâ€‹â€‹â€Œâ€‹â€â€‹â€‹â€Œâ€Œâ€‹â€Œâ€Œâ€‹â€â€‹â€‹â€Œâ€Œâ€‹â€‹â€‹â€‹â€â€‹â€‹â€Œâ€Œâ€‹â€‹â€Œâ€‹â€â€‹â€‹â€Œâ€Œâ€‹â€‹â€‹â€Œâ€â€‹â€‹â€Œâ€Œâ€‹â€‹â€Œâ€Œâ€â€‹â€‹â€Œâ€Œâ€Œâ€‹â€Œâ€‹â€â€‹â€‹â€Œâ€Œâ€Œâ€‹â€‹â€Œâ€â€‹â€‹â€Œâ€Œâ€Œâ€‹â€‹â€‹â€â€‹â€Œâ€Œâ€‹â€‹â€Œâ€‹â€Œâ€â€‹â€‹â€Œâ€Œâ€‹â€‹â€‹â€Œâ€â€‹â€‹â€Œâ€Œâ€‹â€Œâ€‹â€‹â€â€‹â€‹â€Œâ€Œâ€‹â€‹â€Œâ€‹â€â€‹â€Œâ€Œâ€‹â€‹â€‹â€‹â€Œâ€â€‹â€Œâ€Œâ€‹â€‹â€‹â€Œâ€‹ï»¿
> **Profesor:** Juan Marcelo Gutierrez Miranda  
> **MetodologÃ­a:** Cursos Avanzados de Big Data, Ciencia de Datos, Desarrollo de aplicaciones con IA & EconometrÃ­a Aplicada.  
> **Hash ID de CertificaciÃ³n:** `4e8d9b1a5f6e7c3d2b1a0f9e8d7c6b5a4f3e2d1c0b9a8f7e6d5c4b3a2f1e0d9c`  
> **Repositorio:** [https://github.com/TodoEconometria/certificaciones](https://github.com/TodoEconometria/certificaciones)  
>
> **REFERENCIA ACADÃ‰MICA:**
>
> - McKinney, W. (2012). Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython. O'Reilly Media.
> - Harris, C. R., et al. (2020). Array programming with NumPy. Nature, 585(7825), 357-362.
> - Pedregosa, F., et al. (2011). Scikit-learn: Machine Learning in Python. JMLR 12, pp. 2825-2830.

---

## ğŸ›ï¸ 1. IntroducciÃ³n al Laboratorio

En el mundo del **Big Data**, la mayor parte de la informaciÃ³n es **no estructurada** (textos, correos, noticias). El reto fundamental es: *Â¿CÃ³mo puede una computadora entender que dos documentos hablan de lo mismo sin leerlos?*

Este ejercicio implementa un pipeline completo de Ciencia de Datos para agrupar automÃ¡ticamente **1,200 documentos** en espaÃ±ol, utilizando una combinaciÃ³n de tÃ©cnicas matemÃ¡ticas avanzadas para transformar el lenguaje humano en estructuras que las mÃ¡quinas pueden procesar.

---

## ğŸ§  2. Fundamentos TeÃ³ricos (Â¿QuÃ© usamos y por quÃ©?)

### A. VectorizaciÃ³n: El Puente entre Texto y MatemÃ¡ticas

Las computadoras no entienden palabras, solo nÃºmeros. Usamos **TF-IDF (Term Frequency - Inverse Document Frequency)** por su capacidad de discernir la relevancia.

- **Â¿QuÃ© es?**: Un valor estadÃ­stico que busca medir quÃ© tan importante es una palabra para un documento dentro de una colecciÃ³n (corpus).
- **Â¿CÃ³mo funciona?**:
    $$TF(t, d) = \frac{\text{Conteo de la palabra } t \text{ en documento } d}{\text{Total de palabras en } d}$$
    $$IDF(t) = \log\left(\frac{\text{Total de documentos}}{\text{Documentos que contienen la palabra } t}\right)$$
- **Â¿Por quÃ© lo usamos?**: A diferencia del simple conteo (Bak-of-Words), el TF-IDF penaliza palabras que aparecen en todos lados (como "el", "que", "es") y premia palabras temÃ¡ticas ("procesador", "inversiÃ³n", "vacuna").

### B. K-Means: El Cerebro del Agrupamiento

Para el **Aprendizaje No Supervisado**, el algoritmo de **K-Means** es el estÃ¡ndar de oro para encontrar patrones sin etiquetas previas.

- **El Proceso**:
    1. Define $k$ puntos aleatorios (centroides).
    2. Asigna cada documento al centroide mÃ¡s cercano (usando distancia euclidiana en el espacio vectorial).
    3. Recalcula el centro del grupo y repite hasta que los grupos se estabilizan.
- **Por quÃ© lo usamos**: Es extremadamente eficiente para grandes volÃºmenes de datos y nos permite segmentar el mercado, noticias o documentos legales de forma automÃ¡tica.

### C. PCA: Visualizando el Hiperespacio

Nuestra matriz TF-IDF tiene cientos de dimensiones (una por cada palabra Ãºnica). El ser humano solo puede ver en 2D o 3D.

- **Â¿QuÃ© significa?**: **Principal Component Analysis** "comprime" la informaciÃ³n. Busca las direcciones (componentes) donde los datos varÃ­an mÃ¡s y proyecta todo sobre ellas.
- **Utilidad DidÃ¡ctica**: Sin PCA, el clustering serÃ­a una lista de nÃºmeros abstractos. Con PCA, podemos "ver" la separaciÃ³n de conceptos en una grÃ¡fica.

---

## ğŸ“Š 3. InterpretaciÃ³n de lo que estamos viendo

Al ejecutar el cÃ³digo, se genera la visualizaciÃ³n `05_visualizacion_clustering.png`.

![Clustering de Documentos](05_visualizacion_clustering.png)

### Â¿CÃ³mo leer este grÃ¡fico?

1. **CercanÃ­a es Similitud**: Dos puntos que estÃ¡n pegados significan documentos que comparten palabras clave y, por lo tanto, temas.
2. **DispersiÃ³n de Clusters**:
    - Si los grupos estÃ¡n muy separados, el algoritmo ha tenido Ã©xito total identificando temas Ãºnicos.
    - Si hay solapamiento, indica que hay documentos que comparten vocabulario de varios temas (ej. un artÃ­culo sobre "TecnologÃ­a en la Salud").
3. **Los Ejes (PCA)**: El Eje X (Componente 1) suele capturar la diferencia mÃ¡s grande entre los temas (ej. tÃ©rminos mÃ©dicos vs tÃ©rminos financieros).

### Palabras Dominantes por Cluster

Cada cluster tiene un "perfil semÃ¡ntico" definido por las palabras que mÃ¡s contribuyen a su identidad:

![Clustering con Palabras Dominantes](05_clustering_palabras_dominantes.png)

### SelecciÃ³n del k Ã“ptimo (Elbow y Silhouette)

Â¿CÃ³mo sabemos cuÃ¡ntos clusters crear? Dos mÃ©todos complementarios:

- **MÃ©todo del Codo (Elbow):** Buscamos el punto donde la inercia deja de decrecer significativamente.
- **Coeficiente de Silueta:** Un valor cercano a 1 indica clusters bien separados.

![Elbow y Silhouette](05_elbow_silhouette.png)

### AnÃ¡lisis de Silueta por Cluster

El anÃ¡lisis granular muestra la cohesiÃ³n interna de cada cluster. Un ancho uniforme indica clusters bien definidos:

![Silhouette Detallado](05_silhouette_detallado.png)

### ValidaciÃ³n: Temas Reales vs Clusters

Los diagramas de Venn muestran la coincidencia entre los temas reales del corpus y los clusters detectados automÃ¡ticamente. Un solapamiento del 100% confirma que K-Means ha identificado correctamente los tÃ³picos:

![Venn: Temas Reales vs Clusters](05_venn_clusters.png)

---

## ğŸš¢ 4. Caso PrÃ¡ctico: Clustering del Titanic

Para demostrar que K-Means funciona mÃ¡s allÃ¡ del texto, aplicamos el mismo algoritmo al dataset Titanic con caracterÃ­sticas demogrÃ¡ficas (edad, tarifa, clase):

### SelecciÃ³n de k para el Titanic

![Elbow y Silhouette - Titanic](05_titanic_elbow_silhouette.png)

### Clustering de Pasajeros (PCA)

![Clustering Titanic](05_titanic_clustering.png)

### Perfilado de Clusters

Cada cluster revela un perfil de pasajero diferente (edad promedio, tarifa, tasa de supervivencia):

![CaracterÃ­sticas por Cluster](05_titanic_clustering_caracteristicas.png)

---

## ğŸ‘¨â€ğŸ« 5. GuÃ­a DidÃ¡ctica: Paso a Paso

### Paso 1: GeneraciÃ³n del Corpus

Creamos 1,200 documentos sintÃ©ticos. En una formaciÃ³n real, esto simula la ingesta de datos de una API o una base de datos SQL.

### Paso 2: Limpieza y TokenizaciÃ³n

Aunque el script es directo, en NLP real eliminarÃ­amos puntuaciÃ³n, convertirÃ­amos a minÃºsculas y quitarÃ­amos *Stop Words* (palabras vacÃ­as) para que el modelo no se distraiga con ruido.

### Paso 3: Entrenamiento del Modelo

El comando `kmeans.fit(tfidf_matrix)` es donde ocurre la "magia". El modelo "aprende" la estructura latente de los datos sin ayuda humana.

---

## ğŸ“š Referencias y Citas AcadÃ©micas

Para profundizar en la metodologÃ­a, se recomienda la consulta de las siguientes fuentes fundamentales:

- **McKinney, W. (2012).** *Python for Data Analysis*. O'Reilly Media. (Referencia para manipulaciÃ³n de matrices).
- **Pedregosa, F., et al. (2011).** *Scikit-learn: Machine Learning in Python*. JMLR. (DocumentaciÃ³n oficial del framework utilizado).
- **Manning, C. D., et al. (2008).** *Introduction to Information Retrieval*. Cambridge University Press. (TeorÃ­a base de TF-IDF).

ï»¿â€‹â€Œâ€‹â€Œâ€‹â€Œâ€‹â€‹â€â€‹â€Œâ€‹â€‹â€‹â€Œâ€‹â€Œâ€â€‹â€‹â€Œâ€Œâ€Œâ€‹â€Œâ€‹â€â€‹â€‹â€Œâ€Œâ€‹â€Œâ€‹â€‹â€â€‹â€Œâ€Œâ€‹â€‹â€Œâ€‹â€Œâ€â€‹â€‹â€Œâ€Œâ€Œâ€‹â€‹â€‹â€â€‹â€Œâ€Œâ€‹â€‹â€Œâ€‹â€‹â€â€‹â€‹â€Œâ€Œâ€Œâ€‹â€‹â€Œâ€â€‹â€Œâ€Œâ€‹â€‹â€‹â€Œâ€‹â€â€‹â€‹â€Œâ€Œâ€‹â€‹â€‹â€Œâ€â€‹â€Œâ€Œâ€‹â€‹â€‹â€‹â€Œâ€â€‹â€‹â€Œâ€Œâ€‹â€Œâ€‹â€Œâ€â€‹â€Œâ€Œâ€‹â€‹â€Œâ€Œâ€‹â€â€‹â€‹â€Œâ€Œâ€‹â€Œâ€Œâ€‹â€â€‹â€Œâ€Œâ€‹â€‹â€Œâ€‹â€Œâ€â€‹â€‹â€Œâ€Œâ€‹â€Œâ€Œâ€Œâ€â€‹â€Œâ€Œâ€‹â€‹â€‹â€Œâ€Œâ€â€‹â€‹â€Œâ€Œâ€‹â€‹â€Œâ€Œâ€â€‹â€Œâ€Œâ€‹â€‹â€Œâ€‹â€‹â€â€‹â€‹â€Œâ€Œâ€Œâ€‹â€Œâ€‹â€â€‹â€‹â€Œâ€Œâ€‹â€‹â€Œâ€‹â€â€‹â€‹â€Œâ€Œâ€‹â€‹â€‹â€‹â€â€‹â€‹â€Œâ€Œâ€‹â€‹â€Œâ€‹â€â€‹â€‹â€Œâ€Œâ€‹â€Œâ€Œâ€‹â€â€‹â€‹â€Œâ€Œâ€‹â€‹â€‹â€‹â€â€‹â€‹â€Œâ€Œâ€‹â€‹â€Œâ€‹â€â€‹â€‹â€Œâ€Œâ€‹â€‹â€‹â€Œâ€â€‹â€‹â€Œâ€Œâ€‹â€‹â€Œâ€Œâ€â€‹â€‹â€Œâ€Œâ€Œâ€‹â€Œâ€‹â€â€‹â€‹â€Œâ€Œâ€Œâ€‹â€‹â€Œâ€â€‹â€‹â€Œâ€Œâ€Œâ€‹â€‹â€‹â€â€‹â€Œâ€Œâ€‹â€‹â€Œâ€‹â€Œâ€â€‹â€‹â€Œâ€Œâ€‹â€‹â€‹â€Œâ€â€‹â€‹â€Œâ€Œâ€‹â€Œâ€‹â€‹â€â€‹â€‹â€Œâ€Œâ€‹â€‹â€Œâ€‹â€â€‹â€Œâ€Œâ€‹â€‹â€‹â€‹â€Œâ€â€‹â€Œâ€Œâ€‹â€‹â€‹â€Œâ€‹ï»¿---

## ğŸ“ InformaciÃ³n Institucional

**Autor/Referencia:** @TodoEconometria  
**Profesor:** Juan Marcelo Gutierrez Miranda  
**Ãrea:** Big Data, Ciencia de Datos & EconometrÃ­a Aplicada.  

**Hash ID de CertificaciÃ³n:**  
`4e8d9b1a5f6e7c3d2b1a0f9e8d7c6b5a4f3e2d1c0b9a8f7e6d5c4b3a2f1e0d9c`
